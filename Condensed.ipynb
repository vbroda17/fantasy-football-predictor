{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOg1c7H6tAhOW7pXP5b0KI/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fdac24/fantasy-predictions/blob/main/VincentCondensed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nfl_data_py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-grJLlI60RLZ",
        "outputId": "f8de193e-6de7-4fba-a205-c7d426c3c3a2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nfl_data_py\n",
            "  Downloading nfl_data_py-0.3.3-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from nfl_data_py) (1.26.4)\n",
            "Collecting pandas<2.0,>=1.0 (from nfl_data_py)\n",
            "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting appdirs>1 (from nfl_data_py)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting fastparquet>0.5 (from nfl_data_py)\n",
            "  Downloading fastparquet-2024.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting cramjam>=2.3 (from fastparquet>0.5->nfl_data_py)\n",
            "  Downloading cramjam-2.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from fastparquet>0.5->nfl_data_py) (2024.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fastparquet>0.5->nfl_data_py) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.0,>=1.0->nfl_data_py) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.0,>=1.0->nfl_data_py) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas<2.0,>=1.0->nfl_data_py) (1.16.0)\n",
            "Downloading nfl_data_py-0.3.3-py3-none-any.whl (13 kB)\n",
            "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading fastparquet-2024.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cramjam-2.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: appdirs, cramjam, pandas, fastparquet, nfl_data_py\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n",
            "mizani 0.13.0 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "plotnine 0.14.1 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "xarray 2024.10.0 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed appdirs-1.4.4 cramjam-2.9.0 fastparquet-2024.11.0 nfl_data_py-0.3.3 pandas-1.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMCcEZ_e0HRV",
        "outputId": "da55d1ed-5286-4c3b-879f-c58a858ff361"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching active fantasy players...\n",
            "Saved active fantasy players data to nfl_data/active_fantasy_players.csv\n",
            "Active fantasy player fetching complete.\n",
            "Fetching combined game metadata for years [2022, 2023, 2024]...\n",
            "Processing game metadata for 2022...\n",
            "2022 done.\n",
            "Downcasting floats.\n",
            "Processing game metadata for 2023...\n",
            "2023 done.\n",
            "Downcasting floats.\n",
            "Processing game metadata for 2024...\n",
            "2024 done.\n",
            "Downcasting floats.\n",
            "Saved combined game metadata with referees and scores for all years to nfl_data/combined_game_metadata.csv\n",
            "Sample combined game metadata:\n",
            "           game_id home_team away_team  season  week       home_coach  \\\n",
            "0  2022_01_BAL_NYJ       NYJ       BAL    2022     1     Robert Saleh   \n",
            "1   2022_01_BUF_LA        LA       BUF    2022     1       Sean McVay   \n",
            "2  2022_01_CLE_CAR       CAR       CLE    2022     1       Matt Rhule   \n",
            "3  2022_01_DEN_SEA       SEA       DEN    2022     1     Pete Carroll   \n",
            "4   2022_01_GB_MIN       MIN        GB    2022     1  Kevin O'Connell   \n",
            "\n",
            "          away_coach  home_score  away_score winner  \\\n",
            "0      John Harbaugh           9          24    BAL   \n",
            "1     Sean McDermott          10          31    BUF   \n",
            "2    Kevin Stefanski          24          26    CLE   \n",
            "3  Nathaniel Hackett          17          16    SEA   \n",
            "4       Matt LaFleur          23           7    MIN   \n",
            "\n",
            "                                       referee_names  \\\n",
            "0  [Craig Wrolstad, Steve Woods, Jim Mello, Tripp...   \n",
            "1  [Carl Cheffers, Brandon Cruse, Mike Carr, Jeff...   \n",
            "2  [Brad Rogers, Carl Paganelli, Kent Payne, Tom ...   \n",
            "3  [Clete Blakeman, Tab Slaughter, Dana McKenzie,...   \n",
            "4  [Bill Vinovich, Alex Moore, Jerry Bergman, Mar...   \n",
            "\n",
            "            referee_positions  \n",
            "0  [R, U, DJ, LJ, BJ, SJ, FJ]  \n",
            "1  [R, U, DJ, LJ, BJ, SJ, FJ]  \n",
            "2  [R, U, DJ, LJ, BJ, SJ, FJ]  \n",
            "3  [R, U, DJ, LJ, BJ, SJ, FJ]  \n",
            "4  [R, U, DJ, LJ, BJ, SJ, FJ]  \n",
            "Combined game metadata fetching complete.\n",
            "Fetching weekly game data for years [2022, 2023, 2024]...\n",
            "Fetching weekly game data for years: [2022, 2023, 2024]...\n",
            "Downcasting floats.\n",
            "Warning: 4 rows could not be matched to game metadata.\n",
            "      season  week recent_team opponent_team\n",
            "3430    2023    14         BAL           BAL\n",
            "3432    2023    10         CAR           CAR\n",
            "4852    2023     1         NYJ           NYJ\n",
            "2449    2024     7         JAX           JAX\n",
            "Saved weekly game data with game IDs for 2022_2024 to nfl_data/weekly_game_data_with_ids.csv\n",
            "Weekly game data fetching complete.\n"
          ]
        }
      ],
      "source": [
        "import nfl_data_py as nfl\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Directory to save data files\n",
        "DATA_DIR = \"nfl_data\"\n",
        "\n",
        "# Ensure data directory exists\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    os.makedirs(DATA_DIR)\n",
        "\n",
        "# 1. Fetch active fantasy players for the current year and save their data\n",
        "def fetch_and_save_active_fantasy_players(current_year=2024):\n",
        "    try:\n",
        "        players = nfl.__import_rosters(years=[current_year], release='seasonal')\n",
        "        # Filter for active players and specific fantasy positions\n",
        "        active_fantasy_players = players[\n",
        "            (players['status'] == 'ACT') &\n",
        "            (players['position'].isin(['QB', 'WR', 'RB', 'TE']))\n",
        "        ]\n",
        "\n",
        "        # Drop unnecessary columns\n",
        "        columns_to_drop = [\n",
        "            'depth_chart_position', 'jersey_number', 'college', 'espn_id', 'sportradar_id',\n",
        "            'yahoo_id', 'rotowire_id', 'pff_id', 'pfr_id', 'fantasy_data_id', 'sleeper_id',\n",
        "            'headshot_url', 'ngs_position', 'status_description_abbr', 'football_name',\n",
        "            'esb_id', 'gsis_it_id', 'smart_id', 'entry_year'\n",
        "        ]\n",
        "        active_fantasy_players = active_fantasy_players.drop(columns=columns_to_drop)\n",
        "\n",
        "        # Save the data to CSV\n",
        "        active_fantasy_players.to_csv(f\"{DATA_DIR}/active_fantasy_players.csv\", index=False)\n",
        "        print(f\"Saved active fantasy players data to {DATA_DIR}/active_fantasy_players.csv\")\n",
        "        return active_fantasy_players\n",
        "    except AttributeError as e:\n",
        "        print(\"Error: The nfl_data_py module does not provide a function for rosters.\")\n",
        "        raise e\n",
        "\n",
        "def fetch_and_save_weekly_game_data_with_ids(year_range, active_players):\n",
        "    try:\n",
        "        # Fetch the weekly data for the specified year range\n",
        "        print(f\"Fetching weekly game data for years: {year_range}...\")\n",
        "        weekly_game_data = nfl.import_weekly_data(year_range)\n",
        "\n",
        "        # Filter for only regular season games\n",
        "        weekly_game_data = weekly_game_data[weekly_game_data['season_type'] == 'REG']\n",
        "\n",
        "        # Filter for active players\n",
        "        active_player_ids = set(active_players['player_id'])\n",
        "        weekly_game_data = weekly_game_data[weekly_game_data['player_id'].isin(active_player_ids)]\n",
        "\n",
        "        # Drop unnecessary columns\n",
        "        columns_to_drop = ['player_name', 'position_group', 'headshot_url']\n",
        "        weekly_game_data = weekly_game_data.drop(columns=columns_to_drop)\n",
        "\n",
        "        # Load the combined game metadata file\n",
        "        combined_metadata_file = f\"{DATA_DIR}/combined_game_metadata.csv\"\n",
        "        game_metadata = pd.read_csv(combined_metadata_file)\n",
        "\n",
        "        # Perform matching based on season, week, and team involvement\n",
        "        def enrich_row_with_metadata(row):\n",
        "            season = row['season']\n",
        "            week = row['week']\n",
        "            team1 = row['recent_team']\n",
        "            team2 = row['opponent_team']\n",
        "\n",
        "            # Find matching games in the metadata\n",
        "            match = game_metadata[\n",
        "                (game_metadata['season'] == season) &\n",
        "                (game_metadata['week'] == week) &\n",
        "                (\n",
        "                    ((game_metadata['home_team'] == team1) & (game_metadata['away_team'] == team2)) |\n",
        "                    ((game_metadata['home_team'] == team2) & (game_metadata['away_team'] == team1))\n",
        "                )\n",
        "            ]\n",
        "\n",
        "            # Enrich row if a match is found\n",
        "            if not match.empty:\n",
        "                enriched_data = match.iloc[0]\n",
        "                return pd.Series({\n",
        "                    'game_id': enriched_data['game_id'],\n",
        "                    'home_team': enriched_data['home_team'],\n",
        "                    'away_team': enriched_data['away_team'],\n",
        "                    'home_coach': enriched_data['home_coach'],\n",
        "                    'away_coach': enriched_data['away_coach'],\n",
        "                    'winner': enriched_data['winner'],\n",
        "                    'referee_names': enriched_data['referee_names']\n",
        "                })\n",
        "            return pd.Series({\n",
        "                'game_id': None,\n",
        "                'home_team': None,\n",
        "                'away_team': None,\n",
        "                'home_coach': None,\n",
        "                'away_coach': None,\n",
        "                'winner': None,\n",
        "                'referee_names': None\n",
        "            })\n",
        "\n",
        "        # Apply the matching function to enrich the data\n",
        "        enriched_metadata = weekly_game_data.apply(enrich_row_with_metadata, axis=1)\n",
        "\n",
        "        # Combine the enriched metadata with the original weekly game data\n",
        "        weekly_game_data = pd.concat([weekly_game_data, enriched_metadata], axis=1)\n",
        "\n",
        "        # Log missing game_id entries for debugging\n",
        "        missing_game_data = weekly_game_data[weekly_game_data['game_id'].isna()]\n",
        "        if not missing_game_data.empty:\n",
        "            print(f\"Warning: {len(missing_game_data)} rows could not be matched to game metadata.\")\n",
        "            print(missing_game_data[['season', 'week', 'recent_team', 'opponent_team']].head())\n",
        "\n",
        "        # Fill missing numerical fields with 0 and categorical fields with \"Unknown\"\n",
        "        numerical_fields = [\n",
        "            'completions', 'attempts', 'passing_yards', 'passing_tds', 'interceptions',\n",
        "            'sacks', 'sack_yards', 'sack_fumbles', 'sack_fumbles_lost', 'passing_air_yards',\n",
        "            'passing_yards_after_catch', 'passing_first_downs', 'passing_epa', 'passing_2pt_conversions',\n",
        "            'pacr', 'dakota', 'carries', 'rushing_yards', 'rushing_tds', 'rushing_fumbles',\n",
        "            'rushing_fumbles_lost', 'rushing_first_downs', 'rushing_epa', 'rushing_2pt_conversions',\n",
        "            'receptions', 'targets', 'receiving_yards', 'receiving_tds', 'receiving_fumbles',\n",
        "            'receiving_fumbles_lost', 'receiving_air_yards', 'receiving_yards_after_catch',\n",
        "            'receiving_first_downs', 'receiving_epa', 'receiving_2pt_conversions', 'racr',\n",
        "            'target_share', 'air_yards_share', 'wopr', 'special_teams_tds', 'fantasy_points', 'fantasy_points_ppr'\n",
        "        ]\n",
        "        weekly_game_data[numerical_fields] = weekly_game_data[numerical_fields].fillna(0)\n",
        "\n",
        "        categorical_fields = ['game_id', 'home_team', 'away_team', 'home_coach', 'away_coach', 'winner', 'referee_names']\n",
        "        weekly_game_data[categorical_fields] = weekly_game_data[categorical_fields].fillna(\"Unknown\")\n",
        "\n",
        "        # Save to CSV\n",
        "        file_range = f\"{year_range[0]}_{year_range[-1]}\" if len(year_range) > 1 else f\"{year_range[0]}\"\n",
        "        weekly_game_data.to_csv(f\"{DATA_DIR}/weekly_game_data_with_ids.csv\", index=False)\n",
        "        print(f\"Saved weekly game data with game IDs for {file_range} to {DATA_DIR}/weekly_game_data_with_ids.csv\")\n",
        "\n",
        "        return weekly_game_data\n",
        "    except Exception as e:\n",
        "        print(\"Error fetching weekly game data with game IDs:\", e)\n",
        "        raise e\n",
        "\n",
        "\n",
        "\n",
        "def match_game_metadata(row, game_metadata):\n",
        "    \"\"\"\n",
        "    Match game metadata based on season, week, and teams.\n",
        "    \"\"\"\n",
        "    season = row['season']\n",
        "    week = row['week']\n",
        "    team1 = row['recent_team']\n",
        "    team2 = row['opponent_team']\n",
        "\n",
        "    # Match game metadata\n",
        "    match = game_metadata[\n",
        "        (game_metadata['season'] == season) &\n",
        "        (game_metadata['week'] == week) &\n",
        "        (\n",
        "            ((game_metadata['home_team'] == team1) & (game_metadata['away_team'] == team2)) |\n",
        "            ((game_metadata['home_team'] == team2) & (game_metadata['away_team'] == team1))\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    if not match.empty:\n",
        "        enriched_data = match.iloc[0]\n",
        "        return pd.Series({\n",
        "            'game_id': enriched_data['game_id'],\n",
        "            'home_team': enriched_data['home_team'],\n",
        "            'away_team': enriched_data['away_team'],\n",
        "            'home_coach': enriched_data['home_coach'],\n",
        "            'away_coach': enriched_data['away_coach'],\n",
        "            'winner': enriched_data['winner'],\n",
        "            'referee_names': enriched_data['referee_names']\n",
        "        })\n",
        "\n",
        "    # Return empty metadata if no match is found\n",
        "    return pd.Series({\n",
        "        'game_id': None,\n",
        "        'home_team': None,\n",
        "        'away_team': None,\n",
        "        'home_coach': None,\n",
        "        'away_coach': None,\n",
        "        'winner': None,\n",
        "        'referee_names': None\n",
        "    })\n",
        "\n",
        "\n",
        "def fetch_and_save_combined_game_metadata(year_range):\n",
        "    try:\n",
        "        # Combine metadata for all years in the range\n",
        "        combined_metadata = []\n",
        "\n",
        "        for year in year_range:\n",
        "            print(f\"Processing game metadata for {year}...\")\n",
        "            pbp_data = nfl.import_pbp_data([year])\n",
        "\n",
        "            # Extract relevant columns for game metadata\n",
        "            columns_to_keep = ['game_id', 'home_team', 'away_team', 'season', 'week', 'home_coach', 'away_coach']\n",
        "            game_metadata = pbp_data[columns_to_keep].drop_duplicates(subset=['game_id'])\n",
        "\n",
        "            # Extract the final scores\n",
        "            final_scores = pbp_data.groupby('game_id').last()[['home_score', 'away_score']].reset_index()\n",
        "\n",
        "            # Merge scores into game metadata\n",
        "            game_metadata = game_metadata.merge(final_scores, on='game_id', how='left')\n",
        "\n",
        "            # Determine the winner\n",
        "            game_metadata['winner'] = game_metadata.apply(\n",
        "                lambda row: row['home_team'] if row['home_score'] > row['away_score'] else (\n",
        "                    row['away_team'] if row['away_score'] > row['home_score'] else 'TIE'\n",
        "                ), axis=1\n",
        "            )\n",
        "\n",
        "            # Fetch the officials data for the specified season\n",
        "            referee_data = nfl.import_officials([year])\n",
        "\n",
        "            # Aggregate referee data by game_id\n",
        "            grouped_referees = referee_data.groupby('game_id').agg({\n",
        "                'name': lambda x: list(x),  # Combine referee names into a list\n",
        "                'off_pos': lambda x: list(x)  # Combine referee positions into a list\n",
        "            }).reset_index()\n",
        "            grouped_referees.rename(columns={'name': 'referee_names', 'off_pos': 'referee_positions'}, inplace=True)\n",
        "\n",
        "            # Merge referee data into game metadata\n",
        "            game_metadata = game_metadata.merge(grouped_referees, on='game_id', how='left')\n",
        "\n",
        "            # Append to the combined list\n",
        "            combined_metadata.append(game_metadata)\n",
        "\n",
        "        # Concatenate all metadata into a single DataFrame\n",
        "        combined_metadata_df = pd.concat(combined_metadata, ignore_index=True)\n",
        "\n",
        "        # Ensure 'season' and 'week' are included\n",
        "        combined_metadata_df = combined_metadata_df[['game_id', 'home_team', 'away_team', 'season', 'week',\n",
        "                                                     'home_coach', 'away_coach', 'home_score', 'away_score',\n",
        "                                                     'winner', 'referee_names', 'referee_positions']]\n",
        "\n",
        "        # Save the combined data to a single file\n",
        "        combined_metadata_file = f\"{DATA_DIR}/combined_game_metadata.csv\"\n",
        "        combined_metadata_df.to_csv(combined_metadata_file, index=False)\n",
        "        print(f\"Saved combined game metadata with referees and scores for all years to {combined_metadata_file}\")\n",
        "\n",
        "        # Print a sample of the combined data for verification\n",
        "        print(\"Sample combined game metadata:\")\n",
        "        print(combined_metadata_df.head())\n",
        "\n",
        "        return combined_metadata_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error fetching and combining game metadata and referee data:\", e)\n",
        "        raise e\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Specify year range\n",
        "    current_year = 2024\n",
        "    game_year_range = list(range(2022, 2025)) # this goes up to the 2024\n",
        "\n",
        "    # Step 1: Fetch active fantasy players\n",
        "    print(\"Fetching active fantasy players...\")\n",
        "    active_fantasy_players = fetch_and_save_active_fantasy_players(current_year)\n",
        "    print(\"Active fantasy player fetching complete.\")\n",
        "\n",
        "    # Step 2: Fetch combined game metadata (including referees and scores)\n",
        "    print(f\"Fetching combined game metadata for years {game_year_range}...\")\n",
        "    fetch_and_save_combined_game_metadata(game_year_range)\n",
        "    print(\"Combined game metadata fetching complete.\")\n",
        "\n",
        "    # Step 3: Fetch weekly game data with game IDs\n",
        "    print(f\"Fetching weekly game data for years {game_year_range}...\")\n",
        "    fetch_and_save_weekly_game_data_with_ids(game_year_range, active_fantasy_players)\n",
        "    print(\"Weekly game data fetching complete.\")\n",
        "\n",
        "main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install pickle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1DsmhoH04rQ",
        "outputId": "2067f02b-0052-43aa-bc92-cee82f31d573"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pickle (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pickle\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# File paths\n",
        "INPUT_FILE = \"nfl_data/weekly_game_data_with_ids.csv\"\n",
        "OUTPUT_FILE = \"nfl_data/processed_data.csv\"\n",
        "\n",
        "# Embedding file paths\n",
        "PLAYER_EMBEDDINGS_FILE = \"nfl_data/player_embeddings.pkl\"\n",
        "TEAM_EMBEDDINGS_FILE = \"nfl_data/team_embeddings.pkl\"\n",
        "\n",
        "def calculate_condensed_stat(row):\n",
        "    \"\"\"\n",
        "    Calculate a condensed stat based on player position using all specified features.\n",
        "    \"\"\"\n",
        "    position = row['position']\n",
        "\n",
        "    if position == 'QB':\n",
        "        return (\n",
        "            row['completions'] * 0.1 +\n",
        "            row['attempts'] * 0.02 +\n",
        "            row['passing_yards'] * 0.04 +\n",
        "            row['passing_tds'] * 4 -\n",
        "            row['interceptions'] * 2 -\n",
        "            row['sacks'] * 0.5 -\n",
        "            row['sack_yards'] * 0.05 +\n",
        "            row['sack_fumbles'] * -0.5 +\n",
        "            row['sack_fumbles_lost'] * -1 +\n",
        "            row['passing_air_yards'] * 0.03 +\n",
        "            row['passing_yards_after_catch'] * 0.02 +\n",
        "            row['passing_first_downs'] * 0.5 +\n",
        "            row['passing_epa'] * 1 +\n",
        "            row['passing_2pt_conversions'] * 2 +\n",
        "            row['pacr'] * 0.1 +\n",
        "            row['dakota'] * 0.2 +\n",
        "            row['carries'] * 0.1 +\n",
        "            row['rushing_yards'] * 0.1 +\n",
        "            row['rushing_tds'] * 6 -\n",
        "            row['rushing_fumbles'] * 2 -\n",
        "            row['rushing_fumbles_lost'] * 2 +\n",
        "            row['rushing_first_downs'] * 0.5 +\n",
        "            row['rushing_epa'] * 1 +\n",
        "            row['rushing_2pt_conversions'] * 2\n",
        "        )\n",
        "    elif position in ['WR', 'TE']:\n",
        "        return (\n",
        "            row['receptions'] * 1 +\n",
        "            row['targets'] * 0.5 +\n",
        "            row['receiving_yards'] * 0.1 +\n",
        "            row['receiving_tds'] * 6 -\n",
        "            row['receiving_fumbles'] * 2 -\n",
        "            row['receiving_fumbles_lost'] * 2 +\n",
        "            row['receiving_air_yards'] * 0.03 +\n",
        "            row['receiving_yards_after_catch'] * 0.02 +\n",
        "            row['receiving_first_downs'] * 0.5 +\n",
        "            row['receiving_epa'] * 1 +\n",
        "            row['receiving_2pt_conversions'] * 2 +\n",
        "            row['racr'] * 0.1 +\n",
        "            row['target_share'] * 0.2 +\n",
        "            row['air_yards_share'] * 0.2 +\n",
        "            row['wopr'] * 0.3\n",
        "        )\n",
        "    elif position == 'RB':\n",
        "        return (\n",
        "            row['carries'] * 0.1 +\n",
        "            row['rushing_yards'] * 0.1 +\n",
        "            row['rushing_tds'] * 6 -\n",
        "            row['rushing_fumbles'] * 2 -\n",
        "            row['rushing_fumbles_lost'] * 2 +\n",
        "            row['rushing_first_downs'] * 0.5 +\n",
        "            row['rushing_epa'] * 1 +\n",
        "            row['rushing_2pt_conversions'] * 2 +\n",
        "            row['receptions'] * 1 +\n",
        "            row['targets'] * 0.5 +\n",
        "            row['receiving_yards'] * 0.1 +\n",
        "            row['receiving_tds'] * 6 -\n",
        "            row['receiving_fumbles'] * 2 -\n",
        "            row['receiving_fumbles_lost'] * 2 +\n",
        "            row['receiving_air_yards'] * 0.03 +\n",
        "            row['receiving_yards_after_catch'] * 0.02 +\n",
        "            row['receiving_first_downs'] * 0.5 +\n",
        "            row['receiving_epa'] * 1 +\n",
        "            row['receiving_2pt_conversions'] * 2 +\n",
        "            row['racr'] * 0.1 +\n",
        "            row['target_share'] * 0.2 +\n",
        "            row['air_yards_share'] * 0.2 +\n",
        "            row['wopr'] * 0.3\n",
        "        )\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "EMBEDDING_DIM = 10\n",
        "\n",
        "def generate_embeddings(data, column, embedding_dim=EMBEDDING_DIM):\n",
        "    \"\"\"\n",
        "    Generate embeddings for a categorical column using PyTorch embeddings.\n",
        "    Condense embeddings into a single list per row.\n",
        "    Also returns a mapping from unique values to embeddings.\n",
        "    \"\"\"\n",
        "    print(f\"Generating embeddings for {column}...\")\n",
        "\n",
        "    # Map unique values to indices\n",
        "    unique_values = data[column].dropna().unique()\n",
        "    value_to_index = {val: idx for idx, val in enumerate(unique_values)}\n",
        "\n",
        "    # Handle missing values in the column\n",
        "    data[column] = data[column].fillna(\"Unknown\")\n",
        "    data[f\"{column}_id\"] = data[column].map(value_to_index)\n",
        "\n",
        "    # Create embedding layer\n",
        "    embedding_layer = nn.Embedding(len(unique_values), embedding_dim)\n",
        "\n",
        "    # Generate embeddings for each value\n",
        "    indices = torch.tensor(data[f\"{column}_id\"].values)\n",
        "    embeddings = embedding_layer(indices).detach().numpy()\n",
        "\n",
        "    # Add embeddings as a single list for each row\n",
        "    data[f\"{column}_embedding\"] = [list(emb) for emb in embeddings]\n",
        "\n",
        "    # Create a mapping from value to embedding\n",
        "    value_embeddings = {}\n",
        "    for val in unique_values:\n",
        "        idx = value_to_index[val]\n",
        "        emb = embedding_layer(torch.tensor(idx)).detach().numpy()\n",
        "        value_embeddings[val] = emb\n",
        "\n",
        "    # Drop the ID column\n",
        "    data = data.drop(columns=[f\"{column}_id\"])\n",
        "    return data, value_embeddings\n",
        "\n",
        "def process_data(input_file, output_file):\n",
        "    \"\"\"\n",
        "    Process the data to calculate condensed stats and embeddings, then save the processed file.\n",
        "    \"\"\"\n",
        "    print(f\"Reading data from {input_file}...\")\n",
        "    data = pd.read_csv(input_file)\n",
        "    print(f\"Data shape: {data.shape}\")\n",
        "\n",
        "    # Fill missing numerical fields with 0\n",
        "    data.fillna(0, inplace=True)\n",
        "\n",
        "    # Calculate the condensed stat\n",
        "    print(\"Calculating condensed stats...\")\n",
        "    data['condensed_stat'] = data.apply(calculate_condensed_stat, axis=1)\n",
        "\n",
        "    # Drop all features used in the condensed stat calculations\n",
        "    features_to_remove = [\n",
        "        'completions', 'attempts', 'passing_yards', 'passing_tds', 'interceptions', 'sacks',\n",
        "        'sack_yards', 'sack_fumbles', 'sack_fumbles_lost', 'passing_air_yards',\n",
        "        'passing_yards_after_catch', 'passing_first_downs', 'passing_epa',\n",
        "        'passing_2pt_conversions', 'pacr', 'dakota', 'carries', 'rushing_yards',\n",
        "        'rushing_tds', 'rushing_fumbles', 'rushing_fumbles_lost', 'rushing_first_downs',\n",
        "        'rushing_epa', 'rushing_2pt_conversions', 'receptions', 'targets', 'receiving_yards',\n",
        "        'receiving_tds', 'receiving_fumbles', 'receiving_fumbles_lost', 'receiving_air_yards',\n",
        "        'receiving_yards_after_catch', 'receiving_first_downs', 'receiving_epa',\n",
        "        'receiving_2pt_conversions', 'racr', 'target_share', 'air_yards_share', 'wopr'\n",
        "    ]\n",
        "    features_to_remove = [col for col in features_to_remove if col in data.columns]\n",
        "    data = data.drop(columns=features_to_remove)\n",
        "\n",
        "    # Initialize dictionaries to hold embeddings\n",
        "    embeddings_to_save = {}\n",
        "\n",
        "    # Generate embeddings for categorical features and collect embeddings\n",
        "    for column in ['player_id', 'recent_team', 'opponent_team']:\n",
        "        if column in data.columns:\n",
        "            data, embeddings = generate_embeddings(data, column)\n",
        "            embeddings_to_save[column] = embeddings\n",
        "\n",
        "    # Remove coaches and referees columns\n",
        "    data = data.drop(columns=['home_coach', 'away_coach', 'referee_names'], errors='ignore')\n",
        "\n",
        "    # Save the processed data\n",
        "    print(f\"Saving processed data to {output_file}...\")\n",
        "    data.to_csv(output_file, index=False)\n",
        "    print(\"Processing complete.\")\n",
        "\n",
        "    # Save embeddings to files\n",
        "    print(\"Saving embeddings...\")\n",
        "    if 'player_id' in embeddings_to_save:\n",
        "        with open(PLAYER_EMBEDDINGS_FILE, 'wb') as f:\n",
        "            pickle.dump(embeddings_to_save['player_id'], f)\n",
        "        print(f\"Player embeddings saved to {PLAYER_EMBEDDINGS_FILE}\")\n",
        "\n",
        "    if 'opponent_team' in embeddings_to_save:\n",
        "        with open(TEAM_EMBEDDINGS_FILE, 'wb') as f:\n",
        "            pickle.dump(embeddings_to_save['opponent_team'], f)\n",
        "        print(f\"Team embeddings saved to {TEAM_EMBEDDINGS_FILE}\")\n",
        "\n",
        "\n",
        "process_data(INPUT_FILE, OUTPUT_FILE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUmDR99z0b4N",
        "outputId": "910112df-f4b1-49dd-e0f3-603056d268fa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading data from nfl_data/weekly_game_data_with_ids.csv...\n",
            "Data shape: (9886, 57)\n",
            "Calculating condensed stats...\n",
            "Generating embeddings for player_id...\n",
            "Generating embeddings for recent_team...\n",
            "Generating embeddings for opponent_team...\n",
            "Saving processed data to nfl_data/processed_data.csv...\n",
            "Processing complete.\n",
            "Saving embeddings...\n",
            "Player embeddings saved to nfl_data/player_embeddings.pkl\n",
            "Team embeddings saved to nfl_data/team_embeddings.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ACTIVE_PLAYERS_FILE = 'nfl_data/active_fantasy_players.csv'\n",
        "PLAYER_INFO_FILE = 'nfl_data/player_info.csv'\n",
        "\n",
        "def main():\n",
        "    # Read the data\n",
        "    active_players = pd.read_csv(ACTIVE_PLAYERS_FILE)\n",
        "\n",
        "    # Ensure the necessary columns are present\n",
        "    required_columns = ['player_name', 'team', 'position', 'player_id']\n",
        "    alternative_columns = {\n",
        "        'team': 'recent_team'  # Map 'team' to 'recent_team' if 'team' doesn't exist\n",
        "    }\n",
        "\n",
        "    for i, col in enumerate(required_columns):\n",
        "        if col not in active_players.columns:\n",
        "            if col in alternative_columns:\n",
        "                alt_col = alternative_columns[col]\n",
        "                if alt_col in active_players.columns:\n",
        "                    required_columns[i] = alt_col\n",
        "                else:\n",
        "                    raise ValueError(f\"Column '{col}' or its alternative '{alt_col}' not found in the data.\")\n",
        "            else:\n",
        "                raise ValueError(f\"Column '{col}' not found in the data.\")\n",
        "\n",
        "    # Extract the required columns\n",
        "    player_info = active_players[required_columns]\n",
        "    # Rename columns to standard names\n",
        "    player_info.columns = ['player_name', 'team', 'position', 'player_id']\n",
        "\n",
        "    # Save to CSV\n",
        "    player_info.to_csv(PLAYER_INFO_FILE, index=False)\n",
        "    print(f\"Saved player information to {PLAYER_INFO_FILE}\")\n",
        "\n",
        "    # Interactive lookup. Uncomment this part of the code out if you want to look up player IDs\n",
        "    # while True:\n",
        "    #     player_name_to_lookup = input(\"Enter the player's name to look up (or 'exit' to quit): \")\n",
        "    #     if player_name_to_lookup.lower() == 'exit':\n",
        "    #         break\n",
        "    #     matches = lookup_player(player_name_to_lookup, player_info)\n",
        "    #     if matches is not None:\n",
        "    #         print(matches)\n",
        "    #         print()\n",
        "\n",
        "def lookup_player(name, player_info_df):\n",
        "    \"\"\"\n",
        "    Look up players by name.\n",
        "    Returns a DataFrame with matching player(s).\n",
        "    \"\"\"\n",
        "    # Case-insensitive search\n",
        "    matches = player_info_df[player_info_df['player_name'].str.lower() == name.lower()]\n",
        "\n",
        "    if matches.empty:\n",
        "        print(f\"No players found with the name '{name}'.\")\n",
        "        return None\n",
        "    else:\n",
        "        print(f\"Found {len(matches)} player(s) with the name '{name}':\")\n",
        "        return matches\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJiztteO1FYR",
        "outputId": "529709f3-8229-4a24-9aa9-252102270df9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved player information to nfl_data/player_info.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Directory to save data files\n",
        "DATA_DIR = \"nfl_data\"\n",
        "\n",
        "# Ensure data directory exists\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    os.makedirs(DATA_DIR)\n",
        "\n",
        "def fetch_and_save_schedules(years):\n",
        "    \"\"\"\n",
        "    Fetches the NFL schedules for the specified years and saves them to a CSV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"Fetching NFL schedules for years: {years}...\")\n",
        "        # Fetch the schedules\n",
        "        schedules = nfl.import_schedules(years=years)\n",
        "\n",
        "        # Print available columns\n",
        "        print(\"Available columns in schedules DataFrame:\")\n",
        "        print(schedules.columns.tolist())\n",
        "\n",
        "        # Define the desired columns\n",
        "        desired_columns = [\n",
        "            'season', 'season_type', 'week', 'game_id',\n",
        "            'game_date', 'game_time_eastern',\n",
        "            'home_team', 'away_team', 'site_city', 'site_state', 'result'\n",
        "        ]\n",
        "\n",
        "        # Keep only columns that are available\n",
        "        available_columns = schedules.columns.tolist()\n",
        "        columns_to_keep = [col for col in desired_columns if col in available_columns]\n",
        "\n",
        "        schedules = schedules[columns_to_keep]\n",
        "\n",
        "        # Convert 'game_date' to datetime if it's available\n",
        "        if 'game_date' in schedules.columns:\n",
        "            schedules['game_date'] = pd.to_datetime(schedules['game_date'])\n",
        "\n",
        "        # Sort the schedules by available columns\n",
        "        sort_columns = ['season', 'week']\n",
        "        if 'game_date' in schedules.columns:\n",
        "            sort_columns.append('game_date')\n",
        "        schedules.sort_values(by=sort_columns, inplace=True)\n",
        "\n",
        "        # Save to CSV without years in the filename\n",
        "        schedules_file = os.path.join(DATA_DIR, 'nfl_schedules.csv')\n",
        "        schedules.to_csv(schedules_file, index=False)\n",
        "        print(f\"Saved NFL schedules to {schedules_file}\")\n",
        "\n",
        "        # Print a sample of the data\n",
        "        print(\"Sample NFL schedules data:\")\n",
        "        print(schedules.head())\n",
        "\n",
        "        return schedules\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error fetching NFL schedules:\", e)\n",
        "        raise e\n",
        "\n",
        "def main():\n",
        "    # Specify the years for which you want to fetch schedules\n",
        "    years_to_fetch = [2024]  # Only 2024 as per your request\n",
        "\n",
        "    # Fetch and save the schedules\n",
        "    fetch_and_save_schedules(years_to_fetch)\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBQWB5uZ1kME",
        "outputId": "982d58fa-23c5-4833-fd18-f96de40b8ce7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching NFL schedules for years: [2024]...\n",
            "Available columns in schedules DataFrame:\n",
            "['game_id', 'season', 'game_type', 'week', 'gameday', 'weekday', 'gametime', 'away_team', 'away_score', 'home_team', 'home_score', 'location', 'result', 'total', 'overtime', 'old_game_id', 'gsis', 'nfl_detail_id', 'pfr', 'pff', 'espn', 'ftn', 'away_rest', 'home_rest', 'away_moneyline', 'home_moneyline', 'spread_line', 'away_spread_odds', 'home_spread_odds', 'total_line', 'under_odds', 'over_odds', 'div_game', 'roof', 'surface', 'temp', 'wind', 'away_qb_id', 'home_qb_id', 'away_qb_name', 'home_qb_name', 'away_coach', 'home_coach', 'referee', 'stadium_id', 'stadium']\n",
            "Saved NFL schedules to nfl_data/nfl_schedules.csv\n",
            "Sample NFL schedules data:\n",
            "      season  week          game_id home_team away_team  result\n",
            "6706    2024     1   2024_01_BAL_KC        KC       BAL     7.0\n",
            "6707    2024     1   2024_01_GB_PHI       PHI        GB     5.0\n",
            "6708    2024     1  2024_01_PIT_ATL       ATL       PIT    -8.0\n",
            "6709    2024     1  2024_01_ARI_BUF       BUF       ARI     6.0\n",
            "6710    2024     1  2024_01_TEN_CHI       CHI       TEN     7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# File paths\n",
        "DATA_DIR = 'nfl_data'\n",
        "\n",
        "PROCESSED_DATA_FILE = os.path.join(DATA_DIR, 'processed_data.csv')\n",
        "PLAYER_EMBEDDINGS_FILE = os.path.join(DATA_DIR, 'player_embeddings.pkl')\n",
        "TEAM_EMBEDDINGS_FILE = os.path.join(DATA_DIR, 'team_embeddings.pkl')\n",
        "MODEL_FILE = os.path.join(DATA_DIR, 'fantasy_score_model.pkl')\n",
        "\n",
        "def load_data():\n",
        "    # Load processed data\n",
        "    data = pd.read_csv(PROCESSED_DATA_FILE)\n",
        "\n",
        "    # Load embeddings\n",
        "    with open(PLAYER_EMBEDDINGS_FILE, 'rb') as f:\n",
        "        player_embeddings = pickle.load(f)\n",
        "    with open(TEAM_EMBEDDINGS_FILE, 'rb') as f:\n",
        "        team_embeddings = pickle.load(f)\n",
        "\n",
        "    return data, player_embeddings, team_embeddings\n",
        "\n",
        "def prepare_features(data, player_embeddings, team_embeddings):\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for idx, row in data.iterrows():\n",
        "        player_id = row['player_id']\n",
        "        opponent_team = row['opponent_team']\n",
        "        fantasy_points_ppr = row['fantasy_points_ppr']\n",
        "\n",
        "        player_embedding = player_embeddings.get(player_id)\n",
        "        team_embedding = team_embeddings.get(opponent_team)\n",
        "\n",
        "        if player_embedding is not None and team_embedding is not None:\n",
        "            combined_embedding = np.concatenate([player_embedding, team_embedding])\n",
        "            X.append(combined_embedding)\n",
        "            y.append(fantasy_points_ppr)\n",
        "        else:\n",
        "            # Skip if embeddings are missing\n",
        "            continue\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    return X, y\n",
        "\n",
        "def train_model(X, y):\n",
        "    # Split data into training and validation sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Initialize and train the model\n",
        "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "    print(\"Training the model...\")\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate the model\n",
        "    y_pred = model.predict(X_val)\n",
        "    mse = mean_squared_error(y_val, y_pred)\n",
        "    mae = mean_absolute_error(y_val, y_pred)\n",
        "    print(f\"Validation MSE: {mse:.2f}\")\n",
        "    print(f\"Validation MAE: {mae:.2f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def save_model(model):\n",
        "    with open(MODEL_FILE, 'wb') as f:\n",
        "        pickle.dump(model, f)\n",
        "    print(f\"Model saved to {MODEL_FILE}\")\n",
        "\n",
        "def main():\n",
        "    data, player_embeddings, team_embeddings = load_data()\n",
        "    X, y = prepare_features(data, player_embeddings, team_embeddings)\n",
        "    model = train_model(X, y)\n",
        "    save_model(model)\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibh5jLRn1rID",
        "outputId": "b5db5a35-2230-4ea4-e56b-3977d654ecdd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the model...\n",
            "Validation MSE: 53.57\n",
            "Validation MAE: 5.52\n",
            "Model saved to nfl_data/fantasy_score_model.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# File paths\n",
        "DATA_DIR = 'nfl_data'\n",
        "PREDICTIONS_DIR = 'predictions'\n",
        "os.makedirs(PREDICTIONS_DIR, exist_ok=True)\n",
        "\n",
        "PLAYER_INFO_FILE = os.path.join(DATA_DIR, 'player_info.csv')\n",
        "SCHEDULES_FILE = os.path.join(DATA_DIR, 'nfl_schedules.csv')  # Updated filename\n",
        "PROCESSED_DATA_FILE = os.path.join(DATA_DIR, 'processed_data.csv')\n",
        "PLAYER_EMBEDDINGS_FILE = os.path.join(DATA_DIR, 'player_embeddings.pkl')\n",
        "TEAM_EMBEDDINGS_FILE = os.path.join(DATA_DIR, 'team_embeddings.pkl')\n",
        "MODEL_FILE = os.path.join(DATA_DIR, 'fantasy_score_model.pkl')\n",
        "\n",
        "def predict_fantasy_score(player_id, opponent_team, player_embeddings, team_embeddings, model):\n",
        "    # Retrieve embeddings\n",
        "    player_embedding = player_embeddings.get(player_id)\n",
        "    team_embedding = team_embeddings.get(opponent_team)\n",
        "    if player_embedding is None or team_embedding is None:\n",
        "        # Return None if embeddings are missing\n",
        "        return None\n",
        "    # Combine embeddings\n",
        "    combined_embedding = np.concatenate([player_embedding, team_embedding]).reshape(1, -1)\n",
        "    # Predict\n",
        "    predicted_score = model.predict(combined_embedding)\n",
        "    return predicted_score[0]\n",
        "\n",
        "def predict_weekly_scores(week_number, season_year, player_info, schedules, player_embeddings, team_embeddings, model):\n",
        "    # Filter schedules for the specified week and season\n",
        "    week_schedule = schedules[\n",
        "        (schedules['season'] == season_year) &\n",
        "        (schedules['week'] == week_number)\n",
        "    ]\n",
        "\n",
        "    if week_schedule.empty:\n",
        "        # print(f\"No games scheduled for week {week_number} in {season_year}.\")\n",
        "        return pd.DataFrame()  # Return empty DataFrame\n",
        "\n",
        "    # Create a mapping from team to opponent team\n",
        "    team_opponent_map = {}\n",
        "    for _, game in week_schedule.iterrows():\n",
        "        home_team = game['home_team']\n",
        "        away_team = game['away_team']\n",
        "        team_opponent_map[home_team] = away_team\n",
        "        team_opponent_map[away_team] = home_team\n",
        "\n",
        "    # List to store predictions\n",
        "    predictions = []\n",
        "\n",
        "    # Iterate over all players\n",
        "    for idx, player in player_info.iterrows():\n",
        "        player_id = player['player_id']\n",
        "        player_name = player['player_name']\n",
        "        team = player['team']\n",
        "        position = player['position']\n",
        "\n",
        "        # Get opponent team\n",
        "        opponent_team = team_opponent_map.get(team)\n",
        "        if opponent_team is None:\n",
        "            # No game scheduled for this team in this week\n",
        "            continue\n",
        "\n",
        "        # Predict the fantasy score\n",
        "        predicted_score = predict_fantasy_score(\n",
        "            player_id=player_id,\n",
        "            opponent_team=opponent_team,\n",
        "            player_embeddings=player_embeddings,\n",
        "            team_embeddings=team_embeddings,\n",
        "            model=model\n",
        "        )\n",
        "\n",
        "        if predicted_score is not None:\n",
        "            predictions.append({\n",
        "                'player_id': player_id,\n",
        "                'player_name': player_name,\n",
        "                'team': team,\n",
        "                'position': position,\n",
        "                'opponent_team': opponent_team,\n",
        "                'predicted_ppr': predicted_score,\n",
        "                'season': season_year,\n",
        "                'week': week_number\n",
        "            })\n",
        "\n",
        "    # Create a DataFrame with predictions\n",
        "    predictions_df = pd.DataFrame(predictions)\n",
        "\n",
        "    return predictions_df\n",
        "\n",
        "def output_top_players(predictions_df, top_n=20, top_x=5):\n",
        "    if predictions_df.empty:\n",
        "        print(\"No predictions to display.\")\n",
        "        return\n",
        "\n",
        "    # Sort the predictions by predicted PPR score in descending order\n",
        "    predictions_df.sort_values(by='predicted_ppr', ascending=False, inplace=True)\n",
        "\n",
        "    # Output the top N players overall\n",
        "    print(f\"Top {top_n} Players:\")\n",
        "    print(predictions_df.head(top_n)[['player_name', 'team', 'position', 'opponent_team', 'predicted_ppr']])\n",
        "\n",
        "    print(\"\\nTop Players by Position:\")\n",
        "    # Group by position and output the top X players for each position\n",
        "    positions = predictions_df['position'].unique()\n",
        "    for pos in positions:\n",
        "        pos_df = predictions_df[predictions_df['position'] == pos]\n",
        "        pos_df = pos_df.head(top_x)\n",
        "        print(f\"\\nPosition: {pos}\")\n",
        "        print(pos_df[['player_name', 'team', 'opponent_team', 'predicted_ppr']])\n",
        "\n",
        "def run_test(player_info, schedules, player_embeddings, team_embeddings, model, processed_data):\n",
        "    # Only test on the 2024 season\n",
        "    season_year = 2024\n",
        "\n",
        "    # Determine the weeks with actual data in processed_data for 2024\n",
        "    available_weeks = processed_data[\n",
        "        (processed_data['season'] == season_year) &\n",
        "        (processed_data['fantasy_points_ppr'].notna())\n",
        "    ]['week'].unique()\n",
        "\n",
        "    if len(available_weeks) == 0:\n",
        "        print(f\"No available weeks with actual data for season {season_year}. Exiting.\")\n",
        "        return\n",
        "\n",
        "    max_week = available_weeks.max()\n",
        "    print(f\"Testing on Season {season_year}, Weeks 1 to {max_week}\")\n",
        "\n",
        "    overall_predictions = pd.DataFrame()\n",
        "\n",
        "    for week_number in range(1, int(max_week) + 1):\n",
        "        if week_number not in available_weeks:\n",
        "            # print(f\"No data for week {week_number} in {season_year}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # print(f\"Testing Season {season_year}, Week {week_number}\")\n",
        "\n",
        "        # Filter player info for players who played in this week\n",
        "        week_player_ids = processed_data[\n",
        "            (processed_data['season'] == season_year) &\n",
        "            (processed_data['week'] == week_number)\n",
        "        ]['player_id'].unique()\n",
        "\n",
        "        if len(week_player_ids) == 0:\n",
        "            # print(f\"No player data for week {week_number} in {season_year}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        week_player_info = player_info[player_info['player_id'].isin(week_player_ids)]\n",
        "\n",
        "        # Predict weekly scores\n",
        "        predictions_df = predict_weekly_scores(\n",
        "            week_number=week_number,\n",
        "            season_year=season_year,\n",
        "            player_info=week_player_info,\n",
        "            schedules=schedules,\n",
        "            player_embeddings=player_embeddings,\n",
        "            team_embeddings=team_embeddings,\n",
        "            model=model\n",
        "        )\n",
        "\n",
        "        if predictions_df.empty:\n",
        "            # print(f\"No predictions made for week {week_number} in {season_year}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Merge with actual scores using 'fantasy_points_ppr'\n",
        "        actual_scores = processed_data[\n",
        "            (processed_data['season'] == season_year) &\n",
        "            (processed_data['week'] == week_number)\n",
        "        ][['player_id', 'fantasy_points_ppr']]\n",
        "\n",
        "        # Ensure 'player_id' is in both dataframes\n",
        "        if 'player_id' not in predictions_df.columns or 'player_id' not in actual_scores.columns:\n",
        "            # print(f\"'player_id' not found in predictions or actual scores for week {week_number}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        predictions_df = predictions_df.merge(actual_scores, on='player_id', how='left')\n",
        "        predictions_df.rename(columns={'fantasy_points_ppr': 'actual_ppr'}, inplace=True)\n",
        "\n",
        "        # Append to overall predictions\n",
        "        overall_predictions = pd.concat([overall_predictions, predictions_df], ignore_index=True)\n",
        "\n",
        "    if overall_predictions.empty:\n",
        "        print(\"No predictions were made during the test. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Calculate overall error metrics\n",
        "    y_true = overall_predictions['actual_ppr'].values\n",
        "    y_pred = overall_predictions['predicted_ppr'].values\n",
        "\n",
        "    errors = y_pred - y_true\n",
        "    abs_errors = np.abs(errors)\n",
        "\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    me = np.mean(errors)\n",
        "    std_error = np.std(errors)\n",
        "\n",
        "    print(f\"\\nOverall Test MSE: {mse:.2f}\")\n",
        "    print(f\"Overall Test MAE: {mae:.2f}\")\n",
        "    print(f\"Overall Mean Error: {me:.2f}\")\n",
        "    print(f\"Overall Standard Deviation of Errors: {std_error:.2f}\")\n",
        "\n",
        "    # Add errors to the DataFrame\n",
        "    overall_predictions['error'] = errors\n",
        "    overall_predictions['abs_error'] = abs_errors\n",
        "\n",
        "    # Best and worst overall predictions\n",
        "    best_prediction = overall_predictions.loc[overall_predictions['abs_error'].idxmin()]\n",
        "    worst_prediction = overall_predictions.loc[overall_predictions['abs_error'].idxmax()]\n",
        "\n",
        "    print(\"\\nBest Overall Prediction:\")\n",
        "    print(best_prediction[['player_name', 'team', 'position', 'predicted_ppr', 'actual_ppr', 'error']])\n",
        "\n",
        "    print(\"\\nWorst Overall Prediction:\")\n",
        "    print(worst_prediction[['player_name', 'team', 'position', 'predicted_ppr', 'actual_ppr', 'error']])\n",
        "\n",
        "    # Group by position and calculate statistics\n",
        "    positions = overall_predictions['position'].unique()\n",
        "\n",
        "    for pos in positions:\n",
        "        pos_data = overall_predictions[overall_predictions['position'] == pos].copy()\n",
        "        y_true_pos = pos_data['actual_ppr'].values\n",
        "        y_pred_pos = pos_data['predicted_ppr'].values\n",
        "        errors_pos = y_pred_pos - y_true_pos\n",
        "        abs_errors_pos = np.abs(errors_pos)\n",
        "\n",
        "        mse_pos = mean_squared_error(y_true_pos, y_pred_pos)\n",
        "        mae_pos = mean_absolute_error(y_true_pos, y_pred_pos)\n",
        "        me_pos = np.mean(errors_pos)\n",
        "        std_error_pos = np.std(errors_pos)\n",
        "\n",
        "        print(f\"\\nPosition: {pos}\")\n",
        "        print(f\" - MSE: {mse_pos:.2f}\")\n",
        "        print(f\" - MAE: {mae_pos:.2f}\")\n",
        "        print(f\" - Mean Error: {me_pos:.2f}\")\n",
        "        print(f\" - Standard Deviation of Errors: {std_error_pos:.2f}\")\n",
        "\n",
        "        # Best and worst predictions for this position\n",
        "        pos_data['error'] = errors_pos\n",
        "        pos_data['abs_error'] = abs_errors_pos\n",
        "\n",
        "        best_pred_pos = pos_data.loc[pos_data['abs_error'].idxmin()]\n",
        "        worst_pred_pos = pos_data.loc[pos_data['abs_error'].idxmax()]\n",
        "\n",
        "        print(f\" - Best Prediction:\")\n",
        "        print(best_pred_pos[['player_name', 'team', 'predicted_ppr', 'actual_ppr', 'error']])\n",
        "\n",
        "        print(f\" - Worst Prediction:\")\n",
        "        print(worst_pred_pos[['player_name', 'team', 'predicted_ppr', 'actual_ppr', 'error']])\n",
        "\n",
        "    # Save overall predictions to CSV\n",
        "    overall_predictions.to_csv(os.path.join(PREDICTIONS_DIR, 'test_predictions.csv'), index=False)\n",
        "    # print(f\"Test predictions saved to {os.path.join(PREDICTIONS_DIR, 'test_predictions.csv')}\")\n",
        "\n",
        "def main():\n",
        "    # Load active players\n",
        "    player_info = pd.read_csv(PLAYER_INFO_FILE)\n",
        "\n",
        "    # Load schedules\n",
        "    schedules = pd.read_csv(SCHEDULES_FILE)\n",
        "\n",
        "    # Load embeddings\n",
        "    with open(PLAYER_EMBEDDINGS_FILE, 'rb') as f:\n",
        "        player_embeddings = pickle.load(f)\n",
        "    with open(TEAM_EMBEDDINGS_FILE, 'rb') as f:\n",
        "        team_embeddings = pickle.load(f)\n",
        "\n",
        "    # Load the trained model\n",
        "    with open(MODEL_FILE, 'rb') as f:\n",
        "        model = pickle.load(f)\n",
        "\n",
        "    # Load processed data for actual scores\n",
        "    processed_data = pd.read_csv(PROCESSED_DATA_FILE)\n",
        "\n",
        "    # Ensure correct data types\n",
        "    processed_data['season'] = processed_data['season'].astype(int)\n",
        "    processed_data['week'] = processed_data['week'].astype(int)\n",
        "\n",
        "    # Specify the week number or 'TEST'\n",
        "    week_input = input(\"Enter the week number for which you want to predict scores (or type 'TEST' to evaluate the model): \")\n",
        "\n",
        "    if week_input.strip().upper() == 'TEST':\n",
        "        # Run the testing routine\n",
        "        run_test(player_info, schedules, player_embeddings, team_embeddings, model, processed_data)\n",
        "    else:\n",
        "        # Proceed with predicting for a specific week\n",
        "        try:\n",
        "            week_number = int(week_input)\n",
        "            season_year = 2024  # Update as needed\n",
        "\n",
        "            # Predict weekly scores\n",
        "            predictions_df = predict_weekly_scores(\n",
        "                week_number=week_number,\n",
        "                season_year=season_year,\n",
        "                player_info=player_info,\n",
        "                schedules=schedules,\n",
        "                player_embeddings=player_embeddings,\n",
        "                team_embeddings=team_embeddings,\n",
        "                model=model\n",
        "            )\n",
        "\n",
        "            if predictions_df.empty:\n",
        "                print(f\"No predictions available for week {week_number} in {season_year}.\")\n",
        "            else:\n",
        "                # Output the top players\n",
        "                output_top_players(predictions_df, top_n=20, top_x=5)\n",
        "\n",
        "                # Save predictions to the 'predictions' folder\n",
        "                predictions_file = os.path.join(PREDICTIONS_DIR, f'predictions_week_{week_number}_{season_year}.csv')\n",
        "                predictions_df.to_csv(predictions_file, index=False)\n",
        "                print(f'Predictions saved to {predictions_file}')\n",
        "\n",
        "        except ValueError:\n",
        "            print(\"Invalid input. Please enter a valid week number or 'TEST'.\")\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgrvmeGQ15cV",
        "outputId": "ea010073-56d8-4d0a-a5db-b790d671a3a6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the week number for which you want to predict scores (or type 'TEST' to evaluate the model): TEST\n",
            "Testing on Season 2024, Weeks 1 to 11\n",
            "\n",
            "Overall Test MSE: 22.13\n",
            "Overall Test MAE: 3.26\n",
            "Overall Mean Error: 0.21\n",
            "Overall Standard Deviation of Errors: 4.70\n",
            "\n",
            "Best Overall Prediction:\n",
            "player_name      Cedrick Wilson Jr.\n",
            "team                             NO\n",
            "position                         WR\n",
            "predicted_ppr              5.397825\n",
            "actual_ppr                      5.4\n",
            "error                     -0.002175\n",
            "Name: 2101, dtype: object\n",
            "\n",
            "Worst Overall Prediction:\n",
            "player_name      Ja'Marr Chase\n",
            "team                       CIN\n",
            "position                    WR\n",
            "predicted_ppr        20.250524\n",
            "actual_ppr                55.4\n",
            "error               -35.149476\n",
            "Name: 2436, dtype: object\n",
            "\n",
            "Position: QB\n",
            " - MSE: 26.41\n",
            " - MAE: 3.83\n",
            " - Mean Error: 0.18\n",
            " - Standard Deviation of Errors: 5.14\n",
            " - Best Prediction:\n",
            "player_name      Anthony Richardson\n",
            "team                            IND\n",
            "predicted_ppr                9.9002\n",
            "actual_ppr                     9.86\n",
            "error                        0.0402\n",
            "Name: 489, dtype: object\n",
            " - Worst Prediction:\n",
            "player_name      Josh Allen\n",
            "team                    BUF\n",
            "predicted_ppr     29.242367\n",
            "actual_ppr             9.76\n",
            "error             19.482367\n",
            "Name: 335, dtype: object\n",
            "\n",
            "Position: RB\n",
            " - MSE: 21.59\n",
            " - MAE: 3.25\n",
            " - Mean Error: 0.07\n",
            " - Standard Deviation of Errors: 4.65\n",
            " - Best Prediction:\n",
            "player_name      Miles Sanders\n",
            "team                       CAR\n",
            "predicted_ppr        11.509167\n",
            "actual_ppr                11.5\n",
            "error                 0.009167\n",
            "Name: 1842, dtype: object\n",
            " - Worst Prediction:\n",
            "player_name      Alvin Kamara\n",
            "team                       NO\n",
            "predicted_ppr       17.562571\n",
            "actual_ppr               44.0\n",
            "error              -26.437429\n",
            "Name: 313, dtype: object\n",
            "\n",
            "Position: TE\n",
            " - MSE: 12.21\n",
            " - MAE: 2.49\n",
            " - Mean Error: 0.16\n",
            " - Standard Deviation of Errors: 3.49\n",
            " - Best Prediction:\n",
            "player_name      Evan Engram\n",
            "team                     JAX\n",
            "predicted_ppr      11.512083\n",
            "actual_ppr              11.5\n",
            "error               0.012083\n",
            "Name: 2085, dtype: object\n",
            " - Worst Prediction:\n",
            "player_name      Dallas Goedert\n",
            "team                        PHI\n",
            "predicted_ppr           8.39899\n",
            "actual_ppr                 27.0\n",
            "error                 -18.60101\n",
            "Name: 580, dtype: object\n",
            "\n",
            "Position: WR\n",
            " - MSE: 26.11\n",
            " - MAE: 3.48\n",
            " - Mean Error: 0.35\n",
            " - Standard Deviation of Errors: 5.10\n",
            " - Best Prediction:\n",
            "player_name      Cedrick Wilson Jr.\n",
            "team                             NO\n",
            "predicted_ppr              5.397825\n",
            "actual_ppr                      5.4\n",
            "error                     -0.002175\n",
            "Name: 2101, dtype: object\n",
            " - Worst Prediction:\n",
            "player_name      Ja'Marr Chase\n",
            "team                       CIN\n",
            "predicted_ppr        20.250524\n",
            "actual_ppr                55.4\n",
            "error               -35.149476\n",
            "Name: 2436, dtype: object\n"
          ]
        }
      ]
    }
  ]
}